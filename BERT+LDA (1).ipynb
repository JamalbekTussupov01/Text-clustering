{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT+LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stop_words import get_stop_words  # Importing the library for handling stop words\n",
    "from nltk.stem.porter import PorterStemmer  # Importing Porter's stemmer for reducing words to their base form\n",
    "import re  # Importing the regular expressions library\n",
    "import nltk  # Importing the Natural Language Toolkit for text processing (e.g., POS tagging, tokenization)\n",
    "from nltk.tokenize import word_tokenize  # Importing the function for word tokenization\n",
    "from language_detector import detect_language  # Importing the function for language detection\n",
    "\n",
    "import pkg_resources  # Importing to handle package resources\n",
    "from symspellpy import SymSpell, Verbosity  # Importing SymSpell for typo correction and word suggestions\n",
    "\n",
    "# Initializing SymSpell with a max edit distance of 3 and a prefix length of 7\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=3, prefix_length=7)\n",
    "# Loading the frequency dictionary\n",
    "dictionary_path = pkg_resources.resource_filename(\"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "if sym_spell.word_count:  # Checking if the dictionary has been loaded\n",
    "    pass\n",
    "else:\n",
    "    # Loading the dictionary if it hasn't been loaded\n",
    "    sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "\n",
    "\n",
    "def f_base(s):\n",
    "    \"\"\"\n",
    "    :param s: string to be processed\n",
    "    :return: processed string\n",
    "    \"\"\"\n",
    "    # Normalization 1: inserting a period before an uppercase letter if it follows a lowercase letter\n",
    "    s = re.sub(r'([a-z])([A-Z])', r'\\1\\. \\2', s)  # Pre-processing before converting to lowercase\n",
    "\n",
    "    # Normalization 2: converting the text to lowercase\n",
    "    s = s.lower()  # Ensures uniformity for analysis\n",
    "\n",
    "    # Normalization 3: replacing HTML symbols (&gt, &lt) with spaces\n",
    "    s = re.sub(r'&gt|&lt', ' ', s)\n",
    "\n",
    "    # Normalization 4: reducing repeated letters occurring more than twice to one\n",
    "    s = re.sub(r'([a-z])\\1{2,}', r'\\1', s)  # Reduces noise in the text\n",
    "\n",
    "    # Normalization 5: removing repeated non-alphabetic characters occurring more than once\n",
    "    s = re.sub(r'([\\W+])\\1{1,}', r'\\1', s)  # Eliminates excessive punctuation\n",
    "\n",
    "    # Normalization 6: replacing the '*' symbol with a period as a delimiter\n",
    "    s = re.sub(r'\\*|\\W\\*|\\*\\W', '. ', s)  # Replaces characters perceived as separators\n",
    "\n",
    "    # Normalization 7: removing text in parentheses as it is considered less formal\n",
    "    s = re.sub(r'\\(.*?\\)', '. ', s)\n",
    "\n",
    "    # Normalization 8: replacing sequences of punctuation with a single period\n",
    "    s = re.sub(r'\\W+?\\.', '.', s)\n",
    "\n",
    "    # Normalization 9: adding a space after punctuation if followed by a word\n",
    "    s = re.sub(r'(\\.|\\?|!)(\\w)', r'\\1 \\2', s)\n",
    "\n",
    "    # Normalization 10: removing the word 'ing' as it is considered noise\n",
    "    s = re.sub(r' ing ', ' ', s)\n",
    "\n",
    "    # Normalization 11: removing noise related to promotional text\n",
    "    s = re.sub(r'product received for free[.| ]', ' ', s)\n",
    "\n",
    "    # Normalization 12: removing repeated phrases to reduce redundancy\n",
    "    s = re.sub(r'(.{2,}?)\\1{1,}', r'\\1', s)\n",
    "\n",
    "    return s.strip()  # Returns the string without leading or trailing spaces\n",
    "\n",
    "\n",
    "# Function for language detection\n",
    "def f_lan(s):\n",
    "    \"\"\"\n",
    "    :param s: string to be processed\n",
    "    :return: boolean (whether the text is in English or French)\n",
    "    \"\"\"\n",
    "    # Checks the language using a language detector\n",
    "    return detect_language(s) in {'English', 'French'}  # Returns True if the language is English or French\n",
    "\n",
    "\n",
    "# Filtering out punctuation and numbers\n",
    "def f_punct(w_list):\n",
    "    \"\"\"\n",
    "    :param w_list: list of words to be processed\n",
    "    :return: list with punctuation and numbers filtered out\n",
    "    \"\"\"\n",
    "    return [word for word in w_list if word.isalpha()]  # Removes elements that are not words\n",
    "\n",
    "\n",
    "# Selecting only nouns\n",
    "def f_noun(w_list):\n",
    "    \"\"\"\n",
    "    :param w_list: list of words to be processed\n",
    "    :return: list containing only nouns\n",
    "    \"\"\"\n",
    "    return [word for (word, pos) in nltk.pos_tag(w_list) if pos[:2] == 'NN']  # Filters based on 'NN' tags (nouns)\n",
    "\n",
    "\n",
    "# Typo correction\n",
    "def f_typo(w_list):\n",
    "    \"\"\"\n",
    "    :param w_list: list of words to be processed\n",
    "    :return: list with typos corrected\n",
    "    \"\"\"\n",
    "    w_list_fixed = []  # Initializes an empty list for corrected words\n",
    "    for word in w_list:  # Iterates over each word in the list\n",
    "        # Searches for suggestions to correct typos using SymSpell\n",
    "        suggestions = sym_spell.lookup(word, Verbosity.CLOSEST, max_edit_distance=3)\n",
    "        if suggestions:  # If suggestions exist, adds the first suggestion to the list\n",
    "            w_list_fixed.append(suggestions[0].term)\n",
    "        else:\n",
    "            pass  # Skips the word if no suggestions are found\n",
    "    return w_list_fixed  # Returns the list of corrected words\n",
    "\n",
    "\n",
    "# Initializing Porter's stemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "# Stemming\n",
    "def f_stem(w_list):\n",
    "    \"\"\"\n",
    "    :param w_list: list of words to be processed\n",
    "    :return: list with stemmed words\n",
    "    \"\"\"\n",
    "    return [p_stemmer.stem(word) for word in w_list]  # Reduces words to their base forms\n",
    "\n",
    "\n",
    "# Creating an English stop word list\n",
    "en_stop = get_stop_words('en')\n",
    "# Adding additional game-related terms to the stop word list\n",
    "en_stop.append('game')\n",
    "en_stop.append('play')\n",
    "en_stop.append('player')\n",
    "en_stop.append('time')\n",
    "\n",
    "\n",
    "# Removing stop words\n",
    "def f_stopw(w_list):\n",
    "    \"\"\"\n",
    "    Function to remove stop words from the list.\n",
    "    \"\"\"\n",
    "    return [word for word in w_list if word not in en_stop]  # Filters out words that are in the stop word list\n",
    "\n",
    "\n",
    "# Preprocessing text at the sentence level\n",
    "def preprocess_sent(rw):\n",
    "    \"\"\"\n",
    "    Obtains sentence-level preprocessed text from raw input.\n",
    "    :param rw: text to be processed\n",
    "    :return: preprocessed sentence-level text\n",
    "    \"\"\"\n",
    "    s = f_base(rw)  # Normalizes the text\n",
    "    if not f_lan(s):  # Checks if the text is in English or French\n",
    "        return None  # Returns None if the language does not meet the criteria\n",
    "    return s  # Returns the processed string\n",
    "\n",
    "\n",
    "# Preprocessing text at the word level\n",
    "def preprocess_word(s):\n",
    "    \"\"\"\n",
    "    Obtains word-level preprocessed data from processed sentences, \n",
    "    including: removing punctuation, selecting nouns, typo correction, stemming, stop word removal.\n",
    "    :param s: sentence to be processed\n",
    "    :return: word-level preprocessed text\n",
    "    \"\"\"\n",
    "    if not s:  # Checks if the string is not empty\n",
    "        return None\n",
    "    w_list = word_tokenize(s)  # Tokenizes the sentence into words\n",
    "    w_list = f_punct(w_list)  # Removes punctuation and numbers\n",
    "    w_list = f_noun(w_list)  # Selects nouns\n",
    "    w_list = f_typo(w_list)  # Corrects typos\n",
    "    w_list = f_stem(w_list)  # Applies stemming\n",
    "    w_list = f_stopw(w_list)  # Removes stop words\n",
    "\n",
    "    return w_list  # Returns the list of processed words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras  # Importing the Keras library for deep learning model building\n",
    "from keras.layers import Input, Dense  # Importing the Input and Dense layers for constructing neural network models\n",
    "from keras.models import Model  # Importing the Model class for creating the autoencoder model\n",
    "from sklearn.model_selection import train_test_split  # Importing function to split data into training and testing sets\n",
    "import warnings  # Importing the warnings library to manage warnings in code execution\n",
    "warnings.filterwarnings('ignore')  # Suppressing warnings for cleaner output\n",
    "import matplotlib.pyplot as plt  # Importing the Matplotlib library for plotting graphs\n",
    "\n",
    "# Defining the Autoencoder class\n",
    "class Autoencoder:\n",
    "    \"\"\"\n",
    "    Autoencoder class for learning a latent space representation.\n",
    "    The architecture is simplified, consisting of only one hidden layer.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, latent_dim=32, activation='relu', epochs=200, batch_size=128):\n",
    "        \"\"\"\n",
    "        Initializes the Autoencoder object.\n",
    "        :param latent_dim: Dimension of the latent space\n",
    "        :param activation: Activation function to be used in the layers\n",
    "        :param epochs: Number of epochs for training\n",
    "        :param batch_size: Size of the batches during training\n",
    "        \"\"\"\n",
    "        self.latent_dim = latent_dim  # Setting the latent space dimension\n",
    "        self.activation = activation  # Setting the activation function\n",
    "        self.epochs = epochs  # Setting the number of epochs\n",
    "        self.batch_size = batch_size  # Setting the batch size\n",
    "        self.autoencoder = None  # Placeholder for the autoencoder model\n",
    "        self.encoder = None  # Placeholder for the encoder model\n",
    "        self.decoder = None  # Placeholder for the decoder model\n",
    "        self.his = None  # Placeholder for training history\n",
    "\n",
    "    def _compile(self, input_dim):\n",
    "        \"\"\"\n",
    "        Compiles the computational graph for the autoencoder.\n",
    "        :param input_dim: Dimension of the input data\n",
    "        \"\"\"\n",
    "        input_vec = Input(shape=(input_dim,))  # Defining the input layer with the given input dimension\n",
    "        encoded = Dense(self.latent_dim, activation=self.activation)(input_vec)  # Creating the encoding layer\n",
    "        decoded = Dense(input_dim, activation=self.activation)(encoded)  # Creating the decoding layer\n",
    "        \n",
    "        # Building the autoencoder model\n",
    "        self.autoencoder = Model(input_vec, decoded)\n",
    "        \n",
    "        # Creating the encoder model up to the encoded representation\n",
    "        self.encoder = Model(input_vec, encoded)\n",
    "        \n",
    "        # Creating the decoder model using the last layer of the autoencoder\n",
    "        encoded_input = Input(shape=(self.latent_dim,))  # Defining a new input for the decoder\n",
    "        decoder_layer = self.autoencoder.layers[-1]  # Retrieving the last layer (decoder)\n",
    "        self.decoder = Model(encoded_input, decoder_layer(encoded_input))\n",
    "        \n",
    "        # Compiling the autoencoder model with the Adam optimizer and mean squared error loss function\n",
    "        self.autoencoder.compile(optimizer='adam', loss=keras.losses.mean_squared_error)\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Trains the autoencoder on the provided data.\n",
    "        :param X: Input data to train the autoencoder\n",
    "        \"\"\"\n",
    "        if not self.autoencoder:  # Check if the model is not already compiled\n",
    "            self._compile(X.shape[1])  # Compile the model using the input data's feature size\n",
    "        \n",
    "        # Splitting the input data into training and testing sets\n",
    "        X_train, X_test = train_test_split(X)\n",
    "        \n",
    "        # Fitting the autoencoder model to the training data\n",
    "        self.his = self.autoencoder.fit(X_train, X_train,\n",
    "                                        epochs=self.epochs,  # Number of epochs for training\n",
    "                                        batch_size=self.batch_size,  # Batch size for training\n",
    "                                        shuffle=True,  # Shuffling the data before each epoch\n",
    "                                        validation_data=(X_test, X_test),  # Validation data for evaluation\n",
    "                                        verbose=0)  # Silent mode during training\n",
    "        \n",
    "        # Plotting the training and validation loss over epochs\n",
    "        plt.figure(figsize=(10, 6), dpi=350)  # Setting the plot size and resolution\n",
    "        plt.plot(self.his.history['loss'], label='Training Loss')  # Plotting the training loss\n",
    "        plt.plot(self.his.history['val_loss'], label='Validation Loss')  # Plotting the validation loss\n",
    "        plt.title('Autoencoder Training Loss')  # Title of the plot\n",
    "        plt.xlabel('Epochs')  # Label for the x-axis\n",
    "        plt.ylabel('Loss')  # Label for the y-axis\n",
    "        plt.legend()  # Displaying the legend\n",
    "        plt.show()  # Showing the plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer  # Importing TF-IDF vectorizer for text feature extraction\n",
    "from sklearn.cluster import KMeans  # Importing KMeans for clustering\n",
    "from gensim import corpora  # Importing Gensim's corpora for creating a dictionary from the tokenized texts\n",
    "import gensim  # Importing Gensim for topic modeling (LDA)\n",
    "from datetime import datetime  # Importing datetime for timestamping model identifiers\n",
    "from sentence_transformers import SentenceTransformer  # Importing the SentenceTransformer for BERT embeddings\n",
    "import numpy as np  # Importing NumPy for numerical operations\n",
    "from sklearn.model_selection import train_test_split  # Importing train_test_split for splitting datasets\n",
    "from sklearn.metrics import silhouette_score  # Importing silhouette_score for clustering evaluation\n",
    "import keras  # Importing Keras for deep learning models\n",
    "from keras.layers import Input, Dense  # Importing Input and Dense layers for neural network models\n",
    "from keras.models import Model  # Importing Model for defining Keras models\n",
    "\n",
    "# Preprocessing function for text documents\n",
    "def preprocess(docs, samp_size=None):\n",
    "    \"\"\"\n",
    "    Preprocesses the input documents.\n",
    "    :param docs: List of documents (raw text data)\n",
    "    :param samp_size: Number of samples to preprocess (default is 100)\n",
    "    :return: Processed sentences, token lists, and indices of sampled documents\n",
    "    \"\"\"\n",
    "    if not samp_size:  # If samp_size is not provided, set it to 100 by default\n",
    "        samp_size = 100\n",
    "\n",
    "    print('Preprocessing raw texts ...')\n",
    "    n_docs = len(docs)  # Total number of documents\n",
    "    sentences = []  # List for preprocessed sentences at the sentence level\n",
    "    token_lists = []  # List for preprocessed word tokens at the word level\n",
    "    idx_in = []  # List to store indices of selected samples\n",
    "    samp = np.random.choice(n_docs, samp_size)  # Randomly selecting sample indices from the documents\n",
    "    \n",
    "    for i, idx in enumerate(samp):\n",
    "        sentence = preprocess_sent(docs[idx])  # Preprocessing the document at the sentence level\n",
    "        token_list = preprocess_word(sentence)  # Preprocessing the sentence into a list of words\n",
    "        \n",
    "        if token_list:  # If token_list is not empty, append data to the respective lists\n",
    "            idx_in.append(idx)\n",
    "            sentences.append(sentence)\n",
    "            token_lists.append(token_list)\n",
    "        \n",
    "        # Display progress as a percentage\n",
    "        print('{} %'.format(str(np.round((i + 1) / len(samp) * 100, 2))), end='\\r')\n",
    "    \n",
    "    print('Preprocessing raw texts. Done!')\n",
    "    return sentences, token_lists, idx_in  # Return processed data\n",
    "\n",
    "# Class for topic modeling\n",
    "class Topic_Model:\n",
    "    def __init__(self, k=10, method='TFIDF'):\n",
    "        \"\"\"\n",
    "        Initializes the topic modeling object.\n",
    "        :param k: Number of topics to be generated\n",
    "        :param method: Method for topic modeling ('TFIDF', 'LDA', 'BERT', 'LDA_BERT')\n",
    "        \"\"\"\n",
    "        if method not in {'TFIDF', 'LDA', 'BERT', 'LDA_BERT'}:  # Check if the provided method is valid\n",
    "            raise Exception('Invalid method!')\n",
    "        \n",
    "        self.k = k  # Number of topics\n",
    "        self.dictionary = None  # Dictionary object for the corpus\n",
    "        self.corpus = None  # Document-term matrix representation of the corpus\n",
    "        self.cluster_model = None  # Clustering model\n",
    "        self.ldamodel = None  # LDA model\n",
    "        self.vec = {}  # Dictionary to store vector representations for different methods\n",
    "        self.gamma = 15  # Parameter for adjusting the relative importance of LDA in 'LDA_BERT'\n",
    "        self.method = method  # Selected method for topic modeling\n",
    "        self.AE = None  # Autoencoder model (for 'LDA_BERT')\n",
    "        self.id = method + '_' + datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")  # Unique identifier for the model\n",
    "\n",
    "    def vectorize(self, sentences, token_lists, method=None):\n",
    "        \"\"\"\n",
    "        Generates vector representations using the selected method.\n",
    "        :param sentences: List of preprocessed sentences\n",
    "        :param token_lists: List of preprocessed word tokens\n",
    "        :param method: Method for vectorization (default is the object's method)\n",
    "        :return: Vector representation of the documents\n",
    "        \"\"\"\n",
    "        if method is None:  # Set to default method if not specified\n",
    "            method = self.method\n",
    "\n",
    "        # Create a dictionary and a document-term matrix from tokenized documents\n",
    "        self.dictionary = corpora.Dictionary(token_lists)\n",
    "        self.corpus = [self.dictionary.doc2bow(text) for text in token_lists]\n",
    "\n",
    "        if method == 'TFIDF':\n",
    "            print('Getting vector representations for TF-IDF ...')\n",
    "            tfidf = TfidfVectorizer()  # Instantiate TF-IDF vectorizer\n",
    "            vec = tfidf.fit_transform(sentences)  # Fit and transform sentences\n",
    "            print('Getting vector representations for TF-IDF. Done!')\n",
    "            return vec\n",
    "\n",
    "        elif method == 'LDA':\n",
    "            print('Getting vector representations for LDA ...')\n",
    "            if not self.ldamodel:  # If LDA model is not already created, create one\n",
    "                self.ldamodel = gensim.models.ldamodel.LdaModel(\n",
    "                    self.corpus, num_topics=self.k, id2word=self.dictionary, passes=20\n",
    "                )\n",
    "\n",
    "            def get_vec_lda(model, corpus, k):\n",
    "                \"\"\"\n",
    "                Generates LDA vector representation for all documents.\n",
    "                :param model: LDA model\n",
    "                :param corpus: Corpus in a document-term matrix format\n",
    "                :param k: Number of topics\n",
    "                :return: LDA vectors with dimensions (n_docs x n_topics)\n",
    "                \"\"\"\n",
    "                n_doc = len(corpus)\n",
    "                vec_lda = np.zeros((n_doc, k))  # Initialize zero matrix for LDA vectors\n",
    "                for i in range(n_doc):\n",
    "                    # Get the topic distribution for the i-th document\n",
    "                    for topic, prob in model.get_document_topics(corpus[i]):\n",
    "                        vec_lda[i, topic] = prob\n",
    "                return vec_lda\n",
    "\n",
    "            vec = get_vec_lda(self.ldamodel, self.corpus, self.k)\n",
    "            print('Getting vector representations for LDA. Done!')\n",
    "            return vec\n",
    "\n",
    "        elif method == 'BERT':\n",
    "            print('Getting vector representations for BERT ...')\n",
    "            model = SentenceTransformer('bert-base-nli-max-tokens')  # Load pre-trained BERT model\n",
    "            vec = np.array(model.encode(sentences, show_progress_bar=True))  # Encode sentences\n",
    "            print('Getting vector representations for BERT. Done!')\n",
    "            return vec\n",
    "\n",
    "        elif method == 'LDA_BERT':\n",
    "            vec_lda = self.vectorize(sentences, token_lists, method='LDA')  # Generate LDA vectors\n",
    "            vec_bert = self.vectorize(sentences, token_lists, method='BERT')  # Generate BERT vectors\n",
    "            vec_ldabert = np.c_[vec_lda * self.gamma, vec_bert]  # Concatenate weighted LDA and BERT vectors\n",
    "            self.vec['LDA_BERT_FULL'] = vec_ldabert  # Store full concatenated vector\n",
    "\n",
    "            if not self.AE:  # If Autoencoder is not initialized, create and fit one\n",
    "                self.AE = Autoencoder()  # Create Autoencoder object\n",
    "                print('Fitting Autoencoder ...')\n",
    "                self.AE.fit(vec_ldabert)  # Fit the Autoencoder on the concatenated vectors\n",
    "                print('Fitting Autoencoder Done!')\n",
    "            vec = self.AE.encoder.predict(vec_ldabert)  # Use encoder part to get reduced representation\n",
    "            return vec\n",
    "\n",
    "    def fit(self, sentences, token_lists, method=None, m_clustering=None):\n",
    "        \"\"\"\n",
    "        Fits the topic model using the selected method and data.\n",
    "        :param sentences: List of preprocessed sentences\n",
    "        :param token_lists: List of preprocessed word tokens\n",
    "        :param method: Method for fitting (default is the object's method)\n",
    "        :param m_clustering: Clustering algorithm (default is KMeans)\n",
    "        \"\"\"\n",
    "        if method is None:  # Set to default method if not specified\n",
    "            method = self.method\n",
    "        if m_clustering is None:  # Set to KMeans if no clustering method is specified\n",
    "            m_clustering = KMeans\n",
    "\n",
    "        if not self.dictionary:  # Create dictionary and document-term matrix if not already created\n",
    "            self.dictionary = corpora.Dictionary(token_lists)\n",
    "            self.corpus = [self.dictionary.doc2bow(text) for text in token_lists]\n",
    "\n",
    "        if method == 'LDA':\n",
    "            if not self.ldamodel:  # If LDA model is not created, create and fit one\n",
    "                print('Fitting LDA ...')\n",
    "                self.ldamodel = gensim.models.ldamodel.LdaModel(\n",
    "                    self.corpus, num_topics=self.k, id2word=self.dictionary, passes=20\n",
    "                )\n",
    "                print('Fitting LDA Done!')\n",
    "        else:\n",
    "            print('Clustering embeddings ...')\n",
    "            self.cluster_model = m_clustering(self.k)  # Initialize clustering model\n",
    "            self.vec[method] = self.vectorize(sentences, token_lists, method)  # Get vector representations\n",
    "            self.cluster_model.fit(self.vec[method])  # Fit the clustering model on the vectors\n",
    "            print('Clustering embeddings. Done!')\n",
    "\n",
    "    def predict(self, sentences, token_lists, out_of_sample=None):\n",
    "        \"\"\"\n",
    "        Predicts topics for new documents.\n",
    "        :param sentences: List of preprocessed sentences\n",
    "        :param token_lists: List of preprocessed word tokens\n",
    "        :param out_of_sample: Indicates if prediction is for out-of-sample data\n",
    "        :return: Predicted topic labels\n",
    "        \"\"\"\n",
    "        out_of_sample = out_of_sample is not None  # Set default to False if not specified\n",
    "\n",
    "        if out_of_sample:  # Handle out-of-sample prediction\n",
    "            corpus = [self.dictionary.doc2bow(text) for text in token_lists]\n",
    "            if self.method != 'LDA':\n",
    "                vec = self.vectorize(sentences, token_lists)  # Generate vectors for new data\n",
    "                print(vec)\n",
    "        else:\n",
    "            corpus = self.corpus  # Use existing corpus for in-sample prediction\n",
    "            vec = self.vec.get(self.method, None)\n",
    "\n",
    "        if self.method == \"LDA\":  # Prediction for LDA model\n",
    "            lbs = np.array(list(map(lambda x: sorted(\n",
    "                self.ldamodel.get_document_topics(x), key=lambda x: x[1], reverse=True)[0][0], corpus)))\n",
    "        else:  # Prediction for non-LDA methods using clustering\n",
    "            lbs = self.cluster_model.predict(vec)\n",
    "        return lbs  # Return predicted labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Counter  # Importing Counter for counting occurrences of items\n",
    "from gensim.models.coherencemodel import CoherenceModel  # Importing CoherenceModel for evaluating topic coherence\n",
    "from sklearn.metrics import silhouette_score  # Importing silhouette_score for cluster quality evaluation\n",
    "import matplotlib.pyplot as plt  # Importing Matplotlib for data visualization\n",
    "from wordcloud import WordCloud  # Importing WordCloud for generating word cloud images\n",
    "import umap  # Importing UMAP for dimensionality reduction and visualization\n",
    "\n",
    "# Setting up a figure for plotting with a specified size and resolution\n",
    "plt.figure(figsize=(10, 10), dpi=200)\n",
    "\n",
    "# Function for plotting UMAP embeddings\n",
    "def plot_proj(embedding, lbs):\n",
    "    \"\"\"\n",
    "    Plots UMAP embeddings for visualizing clusters.\n",
    "    :param embedding: 2D array representing UMAP (or other) embeddings.\n",
    "    :param lbs: Array of labels for each point.\n",
    "    \"\"\"\n",
    "    n = len(embedding)  # Number of data points\n",
    "    counter = Counter(lbs)  # Count occurrences of each label\n",
    "    for i in range(len(np.unique(lbs))):  # Loop over each unique label\n",
    "        # Plot each cluster with unique label and percentage of total points\n",
    "        plt.plot(embedding[:, 0][lbs == i], embedding[:, 1][lbs == i], '.', alpha=0.5,\n",
    "                 label='cluster {}: {:.2f}%'.format(i, counter[i] / n * 100))\n",
    "    plt.legend()  # Display legend on the plot\n",
    "\n",
    "# Function to compute coherence score for a topic model\n",
    "def get_coherence(model, token_lists, measure='c_v'):\n",
    "    \"\"\"\n",
    "    Computes the coherence score for a given topic model.\n",
    "    :param model: Topic_Model object.\n",
    "    :param token_lists: Tokenized documents.\n",
    "    :param measure: Coherence metric to be used (default is 'c_v').\n",
    "    :return: Coherence score.\n",
    "    \"\"\"\n",
    "    if model.method == 'LDA':  # Check if the method is LDA\n",
    "        # Create a CoherenceModel using the LDA model, tokenized texts, and dictionary\n",
    "        cm = CoherenceModel(model=model.ldamodel, texts=token_lists, corpus=model.corpus, dictionary=model.dictionary,\n",
    "                            coherence=measure)\n",
    "    else:\n",
    "        # Get top words for each topic if the model is not LDA\n",
    "        topics = get_topic_words(model, model.k)\n",
    "        # Create a CoherenceModel using the topics and input data\n",
    "        cm = CoherenceModel(topics=topics, texts=token_lists, corpus=model.corpus, dictionary=model.dictionary,\n",
    "                            coherence=measure)\n",
    "    return cm.get_coherence()  # Return the coherence score\n",
    "\n",
    "# Function to calculate the silhouette score for clustering\n",
    "def get_silhouette(model):\n",
    "    \"\"\"\n",
    "    Computes the silhouette score of the clusters.\n",
    "    :param model: Topic_Model object.\n",
    "    :return: Silhouette score or 'N/A' for LDA models.\n",
    "    \"\"\"\n",
    "    if model.method == 'LDA':  # Silhouette score is not applicable for LDA directly\n",
    "        return 'N/A'\n",
    "    lbs = model.cluster_model.labels_  # Cluster labels\n",
    "    vec = model.vec[model.method]  # Vector representation of documents\n",
    "    return silhouette_score(vec, lbs)  # Compute and return the silhouette score\n",
    "\n",
    "# Function for visualizing the topic model using UMAP embeddings\n",
    "def visualize(model):\n",
    "    \"\"\"\n",
    "    Visualizes the model embeddings using UMAP.\n",
    "    :param model: Topic_Model object.\n",
    "    \"\"\"\n",
    "    if model.method == 'LDA':\n",
    "        print(\"LDA model does not support this type of visualization.\")\n",
    "        return  # Exit function if the method is LDA\n",
    "\n",
    "    reducer = umap.UMAP()  # Initialize UMAP reducer\n",
    "    vec_umap = reducer.fit_transform(model.vec[model.method])  # Fit and transform the vector representation\n",
    "    plot_proj(vec_umap, model.cluster_model.labels_)  # Plot the UMAP projection\n",
    "    \n",
    "    # Scatter plot for UMAP embeddings with color coding by cluster labels\n",
    "    plt.scatter(vec_umap[:, 0], vec_umap[:, 1], c=model.cluster_model.labels_, cmap='Spectral', s=5)\n",
    "    plt.colorbar(boundaries=np.arange(model.k + 1) - 0.5).set_ticks(np.arange(model.k))  # Display color bar for clusters\n",
    "    plt.show()  # Display the plot\n",
    "\n",
    "# Function to extract topic words from an LDA model\n",
    "def get_topic_words(lda_model, num_topics):\n",
    "    \"\"\"\n",
    "    Retrieves top words for each topic in an LDA model.\n",
    "    :param lda_model: Trained LDA model.\n",
    "    :param num_topics: Number of topics.\n",
    "    :return: List of strings containing top words for each topic.\n",
    "    \"\"\"\n",
    "    words = []  # Initialize list to store topic words\n",
    "    for topic_id in range(num_topics):  # Loop through each topic\n",
    "        top_words = [word for word, prop in lda_model.show_topic(topic_id)]  # Extract top words for the topic\n",
    "        words.append(' '.join(top_words))  # Join words into a string and add to the list\n",
    "    return words  # Return list of topic words\n",
    "\n",
    "# Function for generating a word cloud for a specific topic\n",
    "def get_wordcloud(topic_words, topic_num):\n",
    "    \"\"\"\n",
    "    Generates and displays a word cloud for a given topic.\n",
    "    :param topic_words: List of topic words.\n",
    "    :param topic_num: Index of the topic to visualize.\n",
    "    \"\"\"\n",
    "    wordcloud = WordCloud(width=500, height=560, background_color='white', collocations=False).generate(topic_words[topic_num])  # Generate word cloud\n",
    "    plt.figure(figsize=(10, 7))  # Set plot size\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')  # Display word cloud\n",
    "    plt.axis(\"off\")  # Hide axes\n",
    "    plt.show()  # Show plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # Importing pandas for data manipulation and analysis\n",
    "\n",
    "# Loading the data from a CSV file\n",
    "data_path = 'bbc-text.csv'  # Specify the path to the CSV file containing the text data\n",
    "print(\"Загрузка данных...\")  # Print a message indicating that data loading has started\n",
    "data = pd.read_csv(data_path)  # Read the CSV file into a pandas DataFrame\n",
    "print(\"Данные загружены.\")  # Print a message indicating that data loading is complete\n",
    "\n",
    "# Preprocessing the text data\n",
    "sentences, token_lists = preprocess(data['text'])  # Apply the `preprocess` function to the 'text' column\n",
    "print(\"Предварительная обработка текстов завершена.\")  # Print a message indicating the completion of text preprocessing\n",
    "\n",
    "# Specify the method for topic modeling\n",
    "method = 'LDA_BERT'  # Define the method for topic modeling (can be 'LDA', 'TFIDF', 'BERT', or 'LDA_BERT')\n",
    "\n",
    "# Creating and training the topic model\n",
    "print(\"Начало обучения модели тематического моделирования...\")  # Print a message indicating the start of model training\n",
    "model = Topic_Model(k=5, method=method)  # Instantiate a Topic_Model object with 5 topics and the specified method\n",
    "model.fit(sentences, token_lists)  # Fit the topic model using the preprocessed sentences and token lists\n",
    "print(\"Модель тематического моделирования обучена.\")  # Print a message indicating that the model has been trained\n",
    "\n",
    "# Saving the trained model using pickle\n",
    "import pickle  # Importing pickle for saving and loading Python objects\n",
    "\n",
    "# Save the trained model to a file\n",
    "with open(f'{method}_model.pkl', 'wb') as file:  # Open a file in write-binary mode to store the model\n",
    "    pickle.dump(model, file)  # Serialize and save the model to the file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyzing topic coherence and other metrics\n",
    "coherence = get_coherence(model, token_lists, measure='c_v')  # Compute the coherence score using the 'c_v' metric\n",
    "print(f\"Когерентность тем: {coherence}\")  # Display the coherence score for the topics\n",
    "\n",
    "# Visualizing results (e.g., with UMAP and word clouds)\n",
    "print(\"Визуализация результатов...\")  # Print a message indicating the start of visualization\n",
    "visualize(model)  # Call the function to visualize the model's UMAP embeddings\n",
    "\n",
    "# Importing necessary modules for further analysis\n",
    "from gensim.models.coherencemodel import CoherenceModel  # Importing CoherenceModel for evaluating coherence\n",
    "from sklearn.metrics import silhouette_score  # Importing silhouette_score for evaluating clustering quality\n",
    "\n",
    "# Assuming that trained models and evaluation data are available\n",
    "\n",
    "# Coherence analysis for LDA or LDA_BERT models:\n",
    "lda_coherence = CoherenceModel(model=model, texts=token_lists, dictionary=model.dictionary, coherence='u_mass').get_coherence()  # Compute U-Mass coherence\n",
    "lda_cv = CoherenceModel(model=model, texts=token_lists, dictionary=model.dictionary, coherence='c_v').get_coherence()  # Compute C_V coherence\n",
    "\n",
    "# If the method involves clustering (e.g., TF-IDF + Clustering, BERT + Clustering, LDA_BERT + Clustering):\n",
    "labels = model.cluster_model.labels_  # Get the labels assigned to each document by the clustering model\n",
    "silhouette_avg = silhouette_score(model.vec[model.method], labels)  # Compute the silhouette score using the document vectors and labels\n",
    "\n",
    "# Display results\n",
    "print(f\"LDA U-Mass Coherence: {lda_coherence}\")  # Print U-Mass coherence score\n",
    "print(f\"LDA C_V Coherence: {lda_cv}\")  # Print C_V coherence score\n",
    "print(f\"Silhouette Score: {silhouette_avg}\")  # Print the silhouette score\n",
    "\n",
    "# Setting the number of topics for extracting top words\n",
    "num_topics = 5\n",
    "\n",
    "# Retrieving the top words for each topic\n",
    "topic_words = get_topic_words(model.ldamodel, num_topics)  # Call the function to get the top words for each topic\n",
    "\n",
    "# Generate a word cloud for a specific topic, e.g., the first topic\n",
    "get_wordcloud(topic_words, topic_num=0)  # Generate and display the word cloud for the specified topic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary  # Importing Gensim's Dictionary for handling the mapping between words and their IDs\n",
    "from gensim.models import LdaModel  # Importing LdaModel for topic modeling\n",
    "\n",
    "# Defining topic names as a mapping from topic index to human-readable labels\n",
    "topic_names = {0: 'tech', 1: 'business', 2: 'sport', 3: 'entertainment', 4: 'politics'}\n",
    "\n",
    "# Creating a set of test texts categorized into different themes\n",
    "test_texts = [\n",
    "    # Tech\n",
    "    \"The rapid advancement in quantum computing has the potential to revolutionize industries by making data processing significantly faster.\",\n",
    "    \"Emerging technologies such as blockchain and IoT are becoming pivotal in shaping the future landscape of digital transactions and smart homes.\",\n",
    "    \n",
    "    # Business\n",
    "    \"Global markets are increasingly volatile, with trade tensions and geopolitical uncertainties affecting investor sentiment.\",\n",
    "    \"Startups are finding it more challenging to secure funding as venture capitalists tighten their criteria in a post-pandemic economy.\",\n",
    "    \n",
    "    # Sport\n",
    "    \"The sports world is eagerly anticipating the upcoming Olympics, where new records are expected to be set in various disciplines.\",\n",
    "    \"Major League Baseball sees a historic season as a young rookie breaks the long-standing home run record.\",\n",
    "    \n",
    "    # Entertainment\n",
    "    \"The film industry is seeing a shift towards streaming platforms, which are now premiering blockbuster movies directly to consumers at home.\",\n",
    "    \"Virtual reality concerts are gaining popularity, offering an immersive experience for fans to see their favorite artists perform live.\",\n",
    "    \n",
    "    # Politics\n",
    "    \"Election campaigns are increasingly relying on social media to engage with voters, raising concerns about misinformation and data privacy.\",\n",
    "    \"International relations are tense as negotiations stall on climate change initiatives, with major countries failing to agree on emissions targets.\"\n",
    "]\n",
    "\n",
    "# Step 1: Preprocessing the test texts\n",
    "# Apply sentence-level and word-level preprocessing to each text in the test set\n",
    "processed_test_texts = [preprocess_word(preprocess_sent(text)) for text in test_texts]\n",
    "\n",
    "# Step 2: Transform the processed test texts into vectors using the model's dictionary\n",
    "# Convert each processed text into a bag-of-words representation\n",
    "test_corpus = [model.dictionary.doc2bow(text) for text in processed_test_texts]\n",
    "\n",
    "# Step 3: Get the topic distribution for each test text\n",
    "# Check if the method used by the model is 'LDA' or 'LDA_BERT'\n",
    "if model.method in ['LDA', 'LDA_BERT']:\n",
    "    # Obtain the topic distribution for each document in the test corpus\n",
    "    test_topics = [model.ldamodel.get_document_topics(bow) for bow in test_corpus]\n",
    "else:\n",
    "    # Raise an error if the method is not implemented for this type of analysis\n",
    "    raise NotImplementedError(\"Әдістер тек LDA және LDA_BERT арналған\")  # \"Methods only for LDA and LDA_BERT\"\n",
    "\n",
    "# Print the topic distribution for each test text\n",
    "for i, topics_distribution in enumerate(test_topics):\n",
    "    print(f\"\\nМәтін {i+1}:\")  # \"Text {i+1}:\"\n",
    "    for topic, prob in topics_distribution:\n",
    "        # Retrieve the topic name by its index, or label it as \"Unknown\" if not found\n",
    "        topic_name = topic_names.get(topic, f\"Белгісіз мәтін {topic}\")  # \"Unknown text\"\n",
    "        print(f\"Тема '{topic_name}': {prob:.4f}\")  # Print the topic name and its probability\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
